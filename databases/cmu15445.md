# CMU 15-445 notes

### Lecture 1 - Intro to databases

- computers are powerful because they can perform fast and accurate computations on vast amounts of data.

- databases are software which can organize collections of inter-related data that model some aspect of the real world. How do we implement this?

	- flat file database: store relational data as comma seperated values files.
		- inefficient querying if database is too large (have to search line by line)
		- need to parse data for every query
		- how do we deal with duplicates?
		- how do we deal with different threads using the same db file?
		- how do we deal with machine crashes in the middle of updates?
		- what if we the file gets too big for a single machine and we need to split the db per multiple machines?

- A database management system (DBMS) is software that allows creation, querying, updating and administration of databases and takes care of above concerns and more.

- A _data model_ is an abstraction to describe the data living in a database. A _schema_ defines what a certain collection of data looks like.

- Some example data models: relational, key-value, graph, document/object, array/matrix

#### Relational model
- data is abstractly stored as relations
- this data can be accessed using high-level language and DBMS figures out best execution strategy
- the internal storage implementation of data is up to DBMS (tree, index, hash etc.)
- every real-world data entity has _attributes_ which describes the entity. a _relation_ is a set of entities with certain attributes. a _tuple_ is a set of attribute values (or the domain) in the relation.
- n-ary relation = n-column table; tuple = row in relation
- a relation's _primary key_ attribute (or set of attributes) uniquely identifies a single tuple
- a _foregin key_ is a common attribute between different relations that defines relationships between foreign entities of different cardinalites like:
	- one-to-one
	- one-to-many: note that lecturer admits we can just use an array to store the "many" attribute values in the secondary relation which all have the same foregin key found once in the primary relation.
	- many-to-many
- procedural (relational algebra) and non-procedural/declarative (relational calculus) primitives to access/query data inside relations.
- relational algebra operators: SELECT, PROJECTION, UNION, INTERSECTION, DIFFERENCE, PRODUCT, JOIN. The order of operations in a query can affect performance.
- SQL is a declarative de facto standard query language implemenation for relational models.

#### Object/document model
- examples: mongodb, elastic, couchbase, dynamodb
- embed data hierarchy/relations into a single document
- what if we want to query data in the hierarchy from the bottom up?
- example: if we have something like {ARTIST, [ALBUM1, ALBUM2, ALBUM3]}, it is easy to query all the albums of an artist but what if we want to query artists from albums. We would need to replicate the data but with {ALBUM, [ARTIST1, ARTIS2...]} and store both collections. This is a lot of redudant and duplicate data and any changes need to be propagated everywhere.


#### Vector data model
- examples: pinecone, weaviate, marqo, lancedb
- natively stores vectors (1d arrays)
- nearest-neighbor search used to answer queries (instead of exact lookups) and generates ranked list of items (with rank indicating items which are the best match for query)
- used for semantic search on embeddings generated by ml-trained models (chatgpt)

### Lecture 3 & 4 - Database storage

#### Disk-based architecture
- primary storage location of database is non-volatile disk (eg. ssd, hdd, network storage) and manages movement of data between non-volatile and volatile memory (DRAM, caches, registers).
- we assume volatile memory is byte-addressable and non-volatile memory is block-addressable. We want to maximise sequential access of blocks on disk. We are fine with random access of memory in RAM but try to minimise writes to random pages. 
- aside: persistent memory is non-volatile memory with access speed of RAM. eg: some SSDs, Intel Optane.
- System design goals:
	- create illusion that entire database is in memory even if larger than memory available (quite similar to virtual memory)
	- large stalls from writing to disk must be managed carefully to avoid peromfrance degredation
	- maximise sequential access on disk (as mentioned before)
- database files are divided into pages and stored on disk. a buffer pool is maintained in memory for programs to interpret these pages. How does DBMS decide which pages to move in and out of buffer pool?
#### Memory-mapped I/O solution: we leave it to OS which uses _mmap_ to store contents of file into address space of program (virtual memory).

[Are You Sure You Want to Use MMAP in Your Database Management System?](https://www.youtube.com/watch?v=1BRGU_AS25c&ab_channel=CIDRDB) ([paper](https://www.cidrdb.org/cidr2022/papers/p13-crotty.pdf))

- Problem 1: Page fault stalls if required page is not in memory. Maybe we can just allow multiple threads to access mmap files to hide these stalls? This may work well for read-only threads.
- Problem 2: What if there are multiple writer threads? How does the OS ensure transaction safety of writes? It doesn't know anything about the database goals and can flush pages out any time.
- Problem 3: Handling SIGBUS interrupts from invalid page access and we need signal handler throughout system or mechanism to validate pages before access.
- Problem 4: Performance issues due to [OS data structure contention](https://www.youtube.com/watch?v=DJ5u5HrbcMk&list=PLSE8ODhjZXjbj8BMuIrRcacnQh20hmY9g&index=4&ab_channel=CMUDatabaseGroup) (read comment under video) as the OS needs to schedule thread on-the-fly. Our own disk scheduler, on the other hand, can benefit from having a pre-decided scheduler plan from the declarative SQL query.

There are some solutions to these problems like:
- _MAP_PRIVATE_: We can solve the transaction safety problem by using OS's copy-on-write feature so that pages modified by program are copied in phyiscal memory before applying the changes and we retain both versions. A write-ahead-log (WAL) is used to record changes and when a transaction commmits, the WAL is flushed to secondary storage and the final version of the COW-tree is persisted. Note that we may eventually end up with a COW-tree double the size of db file if we never shrink the tree which can done using _mremap_.
- _madvise_: inform the OS of the scheduler plan based on SQL query so it can expect to read certain pages.
- _mlock_: tell the os which memory ranges cannot be paged out
- _msync_: tell the os which memory ranges to flush to disk

Databases which fully use mmap:
- elastic, monetdb, lmdb (funny story in video), mongodb (but they bought wiredtiger storage engine later)

#### Custom storage manager solution: DBMS owns memory management of pages
- blocks on disk represnted using pages which contaian tuples, metadata, indexes, logs etc. Pages can refere to three types in a DBMS:
	- hardware pages: 4kb in size (largest atomic write size)
	- os page: 4kb
	- db page: 512b - 32kb
- to idenitfy pages, we use page id to represent offset in some file for a given page size. each page has header metadata describing page contents.
- How do we manage and organise pages in files on disk?
	- heap file: unordered collection pages with tuples inside pages stored in random order. Easy to find pages if db is a single file (offset = #page * page_size). If multiple files, need metadata to track where pages exist and free space exists: aka _page directory_.
	- tree file
	- sequential / sorted
	- hashing file
- How is data organised inside each page? Assume tuples are stored in row-oriented manner.
	- tuple-oriented: how do we store tuples in each page? Flat, linear storage doesn't work can allow for fast indexing but what if tuple postions change? what if all tuples are not fixed size?
		- _Slotted pages_ is the most common page layout scheme which can solve our issues. A slot array points to the starting offsets of individual tuples. Tuples grow bottom-up from page and slots grow top-down.
	
		How do applications identify tuples from each page? We use Record IDs.
		- Record ID = file_id + page_id + slot_#

		What are the problems with this tuple-oriented storage?
		- Fragmentation: empty unsable slots - pages are not fully utilized.
		- Useless disk I/O as entire page must be fetched update single tuple. If tuples to be updated are on seperate pages, then lots of random disk I/O (worst-case).

	- log-structured (sometime log-strucutre merge tree/LSM): we maintain _log entries/records_ of changes (writes) to tuples in an in-memory buffer and then flush out the changes sequentially to disk when buffer is full.
		- this is only for write operations (PUT, DELETE). read operations still require fetching tuples form disk.
		- periodically compact pages to reduce wasted space.
		
		To read a tuple with given id, we need to scan log record from newest to oldest to find newest record with given id. Is this efficient?
		- solution 1: maintain an index that maps each tuple id to newest log record
		- solution 2: post-compaction, since ordering of log records doesn't matter, we can store them in ways which improve lookup efficiency like SST (Sorted string table). In fact, we can just store the log records in SSTs from inception.
		- LevelDB (Google; early 2000s) implements _leveled compaction_ which maintains sorted log files and periodically coalesces them (sort-merge) into smaller files. RocksDB (Facebook) is a fork of LevelDB.

			[https://github.com/facebook/rocksdb/wiki/Compaction](https://github.com/facebook/rocksdb/wiki/Compaction)
		
			[Name that compaction algorithm, Mark Callaghan, 2018](https://smalldatum.blogspot.com/2018/08/name-that-compaction-algorithm.html)
		
		What are the downsides of log-structured storage?
		- compaction is expensive
		- write-amplication (high disk i/o) from repeatedly moving log records into memory for comapction and then back to disk. Contrast this to tuple-oriented storage where you dont bring tuples into memory unless you need to read. 

	- index-organized (mysql, sqlite): what if dbms could keep tuples sorted automatically using an index?
		- we sort tuples in each page based on key in an index

- How is the data laid out inside a tuple for the DBMS to interpret?
	- the DBMS knows the table schema required to figure out tuple's layout
	- all attributes must be word-aligned to enable fast CPU access and avoid execute two reads to load parts of data and reassemble them instead of single access on word-aligned data.
	- most data types are represented the same way as C/C++
	- variable precision numeric types are allowed and they are faster than fixed precision (because CPUs have registers which support them) but more inaccurate.
	- fixed precision numeric types are more accurate and used when rounding errors are unacceptable. requires sepereate extra implementation and space for additional metadata to ensure accuracy.
	- nulls represented using bitmaps (which represents what attributes are null; stored in header) or special values or null flag.
	- to store values larger than page size, DBMS uses seperate overflow storage pages or external storage blobs.

### Lecture 5 - Storage models & Database Compression

#### Database workloads